Optimizations->

“I’ve introduced several optimizations to improve Spark performance:
  1. Validated and filtered the data with Pandas on EC2 before EMR to reduce Spark load.
  2. Broadcasted the daily exchange rate DataFrame (as it’s small) to avoid full shuffle joins.
  3. Wrote outputs in Parquet, partitioned by date to optimize downstream queries with Athena or Redshift Spectrum.”

Explanation :->

1. Pre-validation Using Pandas Before Spark Transformation :
    I use a Pandas-based pre-validation step to ensure only clean, today's records are passed to Spark. 
    This reduces cluster load and cost. The Spark job assumes input integrity and focuses solely on 
    transformations like normalization, enrichment with exchange rates, and writing final outputs to S3
    or (We use Pandas for quick pre-validation of incoming transaction data, especially for checking nulls and basic integrity rules.)
(problem faced here->
        🔹 Problem 1: Date column is object after read_csv() but it should read as date type correct but it didn't?
        Why: Pandas doesn’t auto-parse dates
        df = pd.read_csv("file.csv", parse_dates=["Date"]) or 
        df['Date'] = pd.to_datetime(df['Date'],dayfirst=True)

        🔹 Problem 2: Used .strftime() and comparison with today failed
        Why: .strftime() converts date to string
        Fix:
        df["Date"] = pd.to_datetime(df["Date"])
        today = pd.to_datetime(date.today())
        df_filtered = df[df["Date"].dt.date == today.date()]

        🔹 Problem 3: Got warning with dayfirst=True
        Why: Format was YYYY-MM-DD, but dayfirst=True expects DD-MM-YYYY
        Fix:
        df["Date"] = pd.to_datetime(df["Date"], format="%Y-%m-%d")

        🔹 Problem 4: Want to format as dd.mm.yy
        Fix (for display only):
        df["Formatted_Date"] = df["Date"].dt.strftime("%d.%m.%y")
        today_str = today.strftime("%d.%m.%y")
)
Also build Incremtal model

 2. Broadcast Join with Exchange Rate Table
“The exchange rates dataset is very small and updated daily, so I use broadcast join in Spark to avoid unnecessary shuffle during the join.”